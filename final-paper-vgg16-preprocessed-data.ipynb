{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Transfer Learning Models\n",
    "## VGG16 based transfer learning technique for comparison with SimpleNet\n",
    "The notebook contains code used for training a transfer learning method using VGG16 model. \n",
    "The model classifies preprocessed retinal OCT scans into three categories: urgent referral, routine referral\n",
    "and normal. **5-fold cross validation** was used to present an accurate estimate of the efficacy of the model \n",
    "on unseen data. The notebooks were run on the kaggle platform for training purposes. Since the 5-folds\n",
    "and initial weights of the deep neural net are random, the accuracies might not match with the accuracies\n",
    "given in the publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "* [1. Importing all modules](###-1.-Importing-all-modules)\n",
    "* [2. Loading train and test data](###-2.-Loading-train-and-test-data)\n",
    "* [3. Generator Classes using Sequence class](###-3.-Generator-Classes-using-Sequence-class)\n",
    "* [4. Functions for building and loading the transfer learning model](###-4.-Functions-for-building-and-loading-the-transfer-learning-model)\n",
    "* [5. Functions for training and cross-validation](###-5.-Functions-for-training-and-cross-validation)\n",
    "* [6. Training the model](###-6.-Training-the-model)\n",
    "* [7. Performance metrics of VGG16 based model](###-7.-Performance-metrics-of-VGG16-based-model)\n",
    "* [8. Performance of model on test set](###-8.-Performance-of-model-on-test-set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.utils import Sequence\n",
    "from cv2 import * #Import functions from OpenCV\n",
    "import cv2\n",
    "import glob\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "from statistics import mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading train and test data\n",
    "\n",
    "Training data is split randomly into 5 folds.\n",
    "\n",
    "**Variable description**\n",
    "- img: corresponds to training images\n",
    "- y: corresponds to training images categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    img = glob.glob(\"../input/preprocessedoctcropped224/preprocessed_oct_cropped/preprocessed_OCT_cropped/train/CNV/*.jpeg\");\n",
    "    img = img + glob.glob(\"../input/preprocessedoctcropped224/preprocessed_oct_cropped/preprocessed_OCT_cropped/train/DME/*.jpeg\");\n",
    "    l = len(img);\n",
    "    y = np.zeros((l,3))\n",
    "    y[:,2] =1\n",
    "    img = img + glob.glob(\"../input/preprocessedoctcropped224/preprocessed_oct_cropped/preprocessed_OCT_cropped/train/DRUSEN/*.jpeg\");\n",
    "    m = len(img);\n",
    "    k = np.zeros((m-l,3));\n",
    "    k[:,1] = 1;\n",
    "    y = np.append(y,k, axis =0);\n",
    "    img = img + glob.glob(\"../input/preprocessedoctcropped224/preprocessed_oct_cropped/preprocessed_OCT_cropped/train/NORMAL/*.jpeg\");\n",
    "    k = np.zeros((len(img)-m,3));\n",
    "    k[:,0] = 1;\n",
    "    y = np.append(y,k, axis =0);\n",
    "    from sklearn.utils import shuffle\n",
    "    img, y = shuffle(img,y)\n",
    "    img, y = shuffle(img,y)\n",
    "    img, y = shuffle(img,y)\n",
    "\n",
    "def kfold_on_train():\n",
    "    from sklearn.model_selection import KFold\n",
    "    ## Training with K-fold cross validation\n",
    "    kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "    kf.get_n_splits(img)\n",
    "\n",
    "    X = np.array(img)\n",
    "    y = np.array(y)\n",
    "    np.savetxt('X.out',X,fmt='%s')\n",
    "    np.savetxt('y.out',y)\n",
    "    i=1\n",
    "    for train_index, test_index in kf.split(img):\n",
    "        trainData = X[train_index]\n",
    "        testData = X[test_index]\n",
    "        trainLabels = y[train_index]\n",
    "        testLabels = y[test_index]\n",
    "        np.savetxt('train_index-'+str(i)+'.out',train_index)\n",
    "        np.savetxt('test_index-'+str(i)+'.out',test_index)\n",
    "        i+=1\n",
    "\n",
    "#reading test images, because can't use validation generator here, as that limits the time\n",
    "def load_test_data():\n",
    "    test_img = []\n",
    "    test_y=[]\n",
    "    for images in glob.glob(\"../input/preprocessedoctcropped224/preprocessed_oct_cropped/preprocessed_OCT_cropped/test/CNV/*.jpeg\"):\n",
    "        n = resize(imread(images), (224, 224,3))\n",
    "        test_img.append(n)\n",
    "        test_y.append(2)    \n",
    "\n",
    "    for images in glob.glob(\"../input/preprocessedoctcropped224/preprocessed_oct_cropped/preprocessed_OCT_cropped/test/DME/*.jpeg\"):\n",
    "        n = resize(imread(images), (224, 224,3))\n",
    "        test_img.append(n)\n",
    "        test_y.append(2)    \n",
    "\n",
    "    for images in glob.glob(\"../input/preprocessedoctcropped224/preprocessed_oct_cropped/preprocessed_OCT_cropped/test/DRUSEN/*.jpeg\"):\n",
    "        n = resize(imread(images), (224, 224,3))\n",
    "        test_img.append(n)\n",
    "        test_y.append(1)    \n",
    "\n",
    "    for images in glob.glob(\"../input/preprocessedoctcropped224/preprocessed_oct_cropped/preprocessed_OCT_cropped/test/NORMAL/*.jpeg\"):\n",
    "        n = resize(imread(images), (224, 224,3))\n",
    "        test_img.append(n)\n",
    "        test_y.append(0)    \n",
    "\n",
    "    test_img = np.asarray(test_img)\n",
    "    test_y = np.asarray(test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generator Classes using Sequence class\n",
    "Since our training dataset is large, we would use the Sequence class to help us feed images in batches for training.\n",
    "Different generator classes are defined for training on four folds and cross-validation on the remaining fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "93cad25f972ed8e4d3edfb8a18ccaaad21157f03",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Training_Generator(Sequence):\n",
    "\n",
    "    def __init__(self, image_filenames, labels, batch_size):\n",
    "        self.image_filenames, self.labels = image_filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.n = len(image_filenames)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_min = idx*self.batch_size\n",
    "        idx_max = np.amin([((idx+1)*self.batch_size),self.n])\n",
    "        batch_x = self.image_filenames[idx_min:idx_max]\n",
    "        batch_y = self.labels[idx_min:idx_max]\n",
    "        X = np.array([\n",
    "            resize(imread(file_name), (224, 224,3))\n",
    "               for file_name in batch_x])\n",
    "        y = np.array(batch_y)\n",
    "        return X,y\n",
    "    \n",
    "class Validation_Generator(Sequence):\n",
    "\n",
    "    def __init__(self, image_filenames, batch_size):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.n = len(image_filenames)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_min = idx*self.batch_size\n",
    "        idx_max = np.amin([((idx+1)*self.batch_size),self.n])\n",
    "        batch_x = self.image_filenames[idx_min:idx_max]\n",
    "        X = np.array([\n",
    "            resize(imread(file_name), (224, 224,3))\n",
    "               for file_name in batch_x])\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Functions for building and loading the transfer learning model\n",
    "\n",
    "For each cross-validation build_model() builds the VGG16 based model whereas load_ith_model() \n",
    "loads the ith cross-validation model from memory for testing and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    #load vgg16 without dense layer and with theano dim ordering\n",
    "    base_model = VGG16(weights='../input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top = False, input_shape = (224,224,3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False                                                                                                                                        \n",
    "    num_classes = 3\n",
    "\n",
    "    x = Flatten()(base_model.output)\n",
    "    predictions = Dense(num_classes, activation = 'softmax')(x)\n",
    "\n",
    "    #create graph of your new model\n",
    "    model = Model(input = base_model.input, output = predictions)\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_ith_model(i):\n",
    "    json_file = open('model-'+str(i)+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights('model-'+str(i)+'.h5')\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Functions for training and cross-validation\n",
    "For each cross-validation i, train_save_validate() trains the model, saves model-i and saves the validation performance\n",
    "metrics on validation fold. perfromance_metrics() derives all performance measures from the confusion matrix whereas\n",
    " print_performance() prints the validation metrics in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_validate(i):\n",
    "    batch_size = 64\n",
    "    train_index = np.genfromtxt('train_index-'+str(i)+'.out').astype(int)\n",
    "    test_index = np.genfromtxt('test_index-'+str(i)+'.out').astype(int)\n",
    "    trainData = X[train_index]\n",
    "    testData = X[test_index]\n",
    "    trainLabels = y[train_index]\n",
    "    testLabels = y[test_index]\n",
    "\n",
    "    trainGenerator = Training_Generator(trainData,trainLabels,batch_size)\n",
    "    valGenerator = Validation_Generator(testData,400)\n",
    "    \n",
    "    model = build_model()\n",
    "    if i ==1:\n",
    "        model.summary()\n",
    "        \n",
    "    model.fit_generator(\n",
    "            generator = trainGenerator,\n",
    "            steps_per_epoch=(len(trainData)//batch_size),\n",
    "            epochs=4, verbose = 2, class_weight = [0.70596402, 4.19022748, 0.74357918])\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    with open('model-'+str(i)+'.json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights('model-'+str(i)+'.h5')\n",
    "    \n",
    "    test_y = np.argmax(testLabels,axis=1)    \n",
    "    ans = np.argmax(model.predict_generator(valGenerator),axis =1)\n",
    "    \n",
    "    # performance metrics  \n",
    "    return performance_metrics(test_y,ans)\n",
    "\n",
    "def performance_metrics(test_y,ans):\n",
    "    acc = accuracy_score(test_y,ans)\n",
    "    print('Accuracy = ',acc)\n",
    "    target_names = ['CLass Normal', 'CLass Early', 'Class Late']\n",
    "    cm = confusion_matrix(test_y, ans) \n",
    "    sens0 = cm[0,0]/(cm[0,0]+cm[0,1]+cm[0,2])\n",
    "    spec0 = (cm[1,1]+cm[1,2]+cm[2,1] +cm[2,2])/((cm[1,1]+cm[1,2]+cm[2,1] +cm[2,2])+(cm[1,0]+cm[2,0]))\n",
    "\n",
    "    sens1 = cm[1,1]/(cm[1,0]+cm[1,1]+cm[1,2])\n",
    "    spec1 = (cm[0,0]+cm[2,0]+cm[0,2] +cm[2,2])/((cm[0,0]+cm[2,0]+cm[0,2] +cm[2,2])+(cm[0,1]+cm[2,1]))\n",
    "\n",
    "    sens2 = cm[2,2]/(cm[2,0]+cm[2,1]+cm[2,2])\n",
    "    spec2 = (cm[0,0]+cm[0,1]+cm[1,0] +cm[1,1])/((cm[0,0]+cm[0,1]+cm[1,0] +cm[1,1])+(cm[0,2]+cm[1,2]))\n",
    "  \n",
    "    rep=classification_report(test_y, ans, target_names=target_names, digits = 4, labels=range(0,3),output_dict=True)\n",
    "    return [acc,sens0,spec0,sens1,spec1,sens2,spec2],cm,rep\n",
    "\n",
    "def print_performance(p):\n",
    "    a = list(p[1][2].keys())\n",
    "    b = list(p[1][2][a[0]].keys())\n",
    "\n",
    "\n",
    "    for i in range (0,1):\n",
    "        s = np.zeros(7)\n",
    "        pn = 0\n",
    "        pe = 0\n",
    "        pl =0\n",
    "        fn =0\n",
    "        fe = 0\n",
    "        fl = 0\n",
    "        rn = 0\n",
    "        re = 0\n",
    "        rl = 0\n",
    "        for j in range(1,6):\n",
    "            s = s + p[j][0]\n",
    "            pn = pn +  p[j][2][a[0]][b[0]]\n",
    "            pe = pe +  p[j][2][a[1]][b[0]]\n",
    "            pl = pl +  p[j][2][a[2]][b[0]]\n",
    "            fn = fn + p[j][2][a[0]][b[2]]\n",
    "            fe = fe + p[j][2][a[1]][b[2]]\n",
    "            fl = fl +  p[j][2][a[2]][b[2]]\n",
    "            rn = rn +  p[j][2][a[0]][b[1]]\n",
    "            re = re +  p[j][2][a[1]][b[1]]\n",
    "            rl = rl +  p[j][2][a[2]][b[1]]\n",
    "\n",
    "        print('\\nacc = ',round(s[0]*20,2))\n",
    "        print(\"              sensitivity  specificity      precision      f1-score\")\n",
    "        print('\\nClass Normal: ',round(s[1]*20,2), \",       \", round(s[2]*20,2), \",       \", round(pn*20,2),\",       \", round(fn*20,2))\n",
    "        print('\\nClass Early:  ',round(s[3]*20,2), \",       \", round(s[4]*20,2), \",       \", round(pe*20,2),\",       \", round(fe*20,2))\n",
    "        print('\\nClass Late:   ',round(s[5]*20,2), \",       \", round(s[6]*20,2), \",       \", round(pl*20,2),\",       \", round(fl*20,2))\n",
    "\n",
    "        print('\\nAverage')\n",
    "        print('\\nsensitivity: ',round(mean([s[1],s[3],s[5]])*20,2), \", specificity: \",round(mean([s[2],s[4],s[6]])*20,2),\n",
    "              \", precision: \", round(mean([pn,pe,pl])*20,2),\", f1-score: \", round(mean([fn,fe,fl])*20,2))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_train_data()\n",
    "kfold_on_train()\n",
    "load_test_data()\n",
    "\n",
    "valid_perf = {}\n",
    "\n",
    "for i in range(1,5):\n",
    "    print(\"\\n====== K Fold Validation step => %d/%d =======\" % (i,5))\n",
    "    \n",
    "    valid_perf[i]= train_save_validate(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Performance metrics of VGG16 based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION ACCURACIES\n",
      "\n",
      "\n",
      "acc =  94.61\n",
      "              sensitivity  specificity      precision      f1-score\n",
      "\n",
      "Class Normal:  98.17 ,        96.35 ,        96.04 ,        97.08\n",
      "\n",
      "Class Early:   69.53 ,        98.45 ,        81.54 ,        73.83\n",
      "\n",
      "Class Late:    95.31 ,        96.3 ,        95.54 ,        95.38\n",
      "\n",
      "Average\n",
      "\n",
      "sensitivity:  87.67 , specificity:  97.04 , precision:  91.04 , f1-score:  88.76\n"
     ]
    }
   ],
   "source": [
    "print(\"VALIDATION ACCURACIES\\n\")\n",
    "print_performance(valid_perf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Performance of model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ACCURACIES\n",
      "\n",
      "\n",
      "acc =  90.14\n",
      "              sensitivity  specificity      precision      f1-score\n",
      "\n",
      "Class Normal:  99.92 ,        94.16 ,        85.23 ,        91.95\n",
      "\n",
      "Class Early:   66.64 ,        99.68 ,        98.73 ,        78.99\n",
      "\n",
      "Class Late:    97.0 ,        89.52 ,        90.45 ,        93.55\n",
      "\n",
      "Average\n",
      "\n",
      "sensitivity:  87.85 , specificity:  94.45 , precision:  91.47 , f1-score:  88.16\n"
     ]
    }
   ],
   "source": [
    "perf = {}\n",
    "\n",
    "for i in range(1,6):\n",
    "    model = build_model_intra(i)      \n",
    "    ans = model.predict(t_img)\n",
    "\n",
    "    # performance metrics  \n",
    "    ans = np.argmax(ans, axis =1)\n",
    "    perf[i]=performance_metrics(t_y,ans)\n",
    "\n",
    "\n",
    "print(\"TEST ACCURACIES\\n\")\n",
    "print_performance(perf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
